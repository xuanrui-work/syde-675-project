{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Discriminative Domain Adaptation (ADDA)\n",
    "\n",
    "This is the implementation for the following paper:\n",
    "```\n",
    "@inproceedings{tzeng2017adversarial,\n",
    "  title={Adversarial discriminative domain adaptation},\n",
    "  author={Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={7167--7176},\n",
    "  year={2017}\n",
    "}\n",
    "```\n",
    "\n",
    "The paper is also available [here](./res/Adversarial%20Discriminative%20Domain%20Adaptation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _base_model\n",
    "import _dataloader_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For logging multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "log_root = pathlib.Path(f'./adda')\n",
    "run_id = 1\n",
    "while True:\n",
    "    log_dir = log_root/f'run{run_id}'\n",
    "    if not log_dir.exists():\n",
    "        break\n",
    "    run_id += 1\n",
    "\n",
    "# use a previous run\n",
    "# log_dir = log_root/'run1'\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Logging to: {log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "\n",
    "Here, we transform both the source and target datasets into the same size, and repeat the channel dimension for grayscale images, such that both datasets have the same input shape to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 32, 32)\n",
    "num_classes = 10\n",
    "\n",
    "loader_helper = _dataloader_helper.MNIST2USPS(image_size=input_shape[1:])\n",
    "src_train, src_val, src_test = loader_helper.get_src_loaders()\n",
    "tgt_train, tgt_val, tgt_test = loader_helper.get_tgt_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some samples from both domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "src_samples = next(iter(src_train))\n",
    "tgt_samples = next(iter(tgt_train))\n",
    "\n",
    "src_images, src_labels = src_samples[0][:num_samples], src_samples[1][:num_samples]\n",
    "tgt_images, tgt_labels = tgt_samples[0][:num_samples], tgt_samples[1][:num_samples]\n",
    "\n",
    "def plot_samples(samples, figsize=(16, 6)):\n",
    "    fig, ax = plt.subplots(1, num_samples, figsize=figsize)\n",
    "    for i in range(num_samples):\n",
    "        ax[i].imshow(np.transpose(samples[i], (1, 2, 0)))\n",
    "        ax[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(src_images)\n",
    "print(src_images.shape)\n",
    "print(src_labels)\n",
    "\n",
    "plot_samples(tgt_images)\n",
    "print(tgt_images.shape)\n",
    "print(tgt_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDA\n",
    "\n",
    "The domain adaptation procedure by ADDA is shown in the figure below:<br>\n",
    "<img src=\"./res/images/adda-overview.png\" width=\"1000\"/>\n",
    "\n",
    "It consists of three main steps:\n",
    "1. Train an entire classifier on the source domain.\n",
    "2. Train a feature encoder on the target domain by adversarial training with the source domain feature encoder.\n",
    "3. Combine the feature encoder on the target domain with the classifier trained on the source domain, to form the overall classifier for the target domain.\n",
    "\n",
    "In Step. 2 above, the adversarial training is done by training a discriminator to distinguish between the source and target domain features, while training the target feature encoder to confuse the discriminator. This is given by the following optimization:\n",
    "$$\\begin{align*}\n",
    "\\min_{D} L_{\\text{adv}_D} (\\mathbf{X}_s, \\mathbf{X}_t, M_s, M_t) &= -\\mathbb{E}_{\\mathbf{x}_s \\sim \\mathbf{X}_s} \\log D(M_s(\\mathbf{x}_s)) - \\mathbb{E}_{\\mathbf{x}_t \\sim \\mathbf{X}_t} \\log (1 - D(M_t(\\mathbf{x}_t))) \\\\\n",
    "\\min_{M_t} L_{\\text{adv}_M} (\\mathbf{X}_s, \\mathbf{X}_t, D) &= -\\mathbb{E}_{\\mathbf{x}_t \\sim \\mathbf{X}_t} \\log D(M_t(\\mathbf{x}_t))\n",
    "\\end{align*}$$\n",
    "where $D$ is the discriminator, $M_s$ and $M_t$ are the feature encoders for the source and target domains respectively, and $\\mathbf{X}_s$ and $\\mathbf{X}_t$ are the source and target domain datasets respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Train the source classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the source classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_model = _base_model.CNNClassifier(\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "summary(src_model, input_size=(5, *input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on source domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, loss_fn):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (x, y) in enumerate(val_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred, _ = model(x)\n",
    "            loss = loss_fn(y_pred, y).item()\n",
    "            n_correct = (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            val_loss += loss * x.shape[0]\n",
    "            val_acc += n_correct\n",
    "    model.train(training)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "    return (val_loss, val_acc)\n",
    "\n",
    "def train_src_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    num_steps=20000,\n",
    "    checkpoint=100,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    train_stats = []\n",
    "    val_stats = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred, _ = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if checkpoint and step % checkpoint == 0:\n",
    "            val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
    "\n",
    "            train_stats += [[step, loss.item()]]\n",
    "            val_stats += [[step, val_loss, val_acc]]\n",
    "\n",
    "            tb_writer.add_scalar('src-only/loss/train', loss.item(), step)\n",
    "            tb_writer.add_scalar('src-only/loss/val', val_loss, step)\n",
    "            tb_writer.add_scalar('src-only/acc/val', val_acc, step)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), log_dir/'src_best_model.pth')\n",
    "    \n",
    "    tb_writer.close()\n",
    "\n",
    "    train_stats = np.array(train_stats)\n",
    "    val_stats = np.array(val_stats)\n",
    "    return (train_stats, val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats, val_stats = train_src_model(\n",
    "    src_model,\n",
    "    src_train,\n",
    "    src_val,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    torch.optim.Adam(src_model.parameters(), lr=1e-3),\n",
    "    num_steps=2000,\n",
    "    checkpoint=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 1], label='train_loss')\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 1], label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 2], label='val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_model.load_state_dict(torch.load(log_dir/'src_best_model.pth'))\n",
    "\n",
    "src_loss, src_acc = evaluate(src_model, src_test, nn.CrossEntropyLoss())\n",
    "tgt_loss, tgt_acc = evaluate(src_model, tgt_test, nn.CrossEntropyLoss())\n",
    "\n",
    "print('Source-only model:')\n",
    "print(f'src_test -> loss: {src_loss:.4f}, acc: {src_acc:.4f}')\n",
    "print(f'tgt_test -> loss: {tgt_loss:.4f}, acc: {tgt_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Train the target encoder\n",
    "\n",
    "The target encoder is initialized with the weights of the source encoder from the source classifier. Then, it is trained with the following adversarial objective:\n",
    "$$\\begin{align*}\n",
    "\\min_{M_t} \\max_{D} \\ \\mathcal{L}_{adv}(M_s, M_t, D) &= \\mathbb{E}_{x_s \\sim p_s(x)}[\\log D(M_s(x_s))] + \\mathbb{E}_{x_t \\sim p_t(x)}[\\log(1 - D(M_t(x_t)))]\n",
    "\\end{align*}$$\n",
    "where $M_s$ and $M_t$ are the source and target encoders respectively, and $D$ is the discriminator. Furthermore, note that in this case we are fixing $M_s$ and only optimizing w.r.t. $M_t$ and $D$.\n",
    "\n",
    "Under this objective, the discriminator $D$ tries to distinguish between the source and target features produced by $M_s$ and $M_t$, while the target encoder $M_t$ tries to fool the discriminator by generating features that align with the source domain features outputted by $M_s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the target encoder, classifier, and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_model = _base_model.CNNClassifier(\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# initialize tgt encoder and classifier with src classifier weights\n",
    "# this initializes both the encoder and classifier (fc layers) in the tgt classifier\n",
    "tgt_model.load_state_dict(src_model.state_dict())\n",
    "\n",
    "# freeze the source model\n",
    "for param in src_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# freeze the target cls_head\n",
    "for param in tgt_model.cls_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# initialize the discriminator\n",
    "discriminator = _base_model.FCHead(\n",
    "    input_shape=tgt_model.encoder.output_shape,\n",
    "    op_out_features=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the target encoder adversarially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tgt_encoder(\n",
    "    src_model,\n",
    "    tgt_model,\n",
    "    discriminator,\n",
    "    src_train_loader,\n",
    "    tgt_train_loader,\n",
    "    tgt_val_loader,\n",
    "    loss_fn,\n",
    "    tgt_model_optim,\n",
    "    discriminator_optim,\n",
    "    num_steps=20000,\n",
    "    checkpoint=100,\n",
    "):\n",
    "    src_model.train()\n",
    "    tgt_model.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    train_stats = []\n",
    "    val_stats = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "    src_train_iter = iter(src_train_loader)\n",
    "    tgt_train_iter = iter(tgt_train_loader)\n",
    "\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        try:\n",
    "            src_x, _ = next(src_train_iter)\n",
    "        except StopIteration:\n",
    "            src_train_iter = iter(src_train_loader)\n",
    "            src_x, _ = next(src_train_iter)\n",
    "        try:\n",
    "            tgt_x, _ = next(tgt_train_iter)\n",
    "        except StopIteration:\n",
    "            tgt_train_iter = iter(tgt_train_loader)\n",
    "            tgt_x, _ = next(tgt_train_iter)\n",
    "        \n",
    "        src_x = src_x.to(device)\n",
    "        tgt_x = tgt_x.to(device)\n",
    "\n",
    "        # udpate the discriminator\n",
    "        _, src_features = src_model(src_x)\n",
    "        _, tgt_features = tgt_model(tgt_x)\n",
    "\n",
    "        # src features are labeled as 1, tgt features are labeled as 0\n",
    "        src_labels = torch.full((src_x.shape[0],), fill_value=1, dtype=torch.long, device=device)\n",
    "        tgt_labels = torch.full((tgt_x.shape[0],), fill_value=0, dtype=torch.long, device=device)\n",
    "\n",
    "        src_preds = discriminator(src_features.detach())\n",
    "        tgt_preds = discriminator(tgt_features.detach())\n",
    "\n",
    "        D_src_loss = loss_fn(src_preds, src_labels)\n",
    "        D_tgt_loss = loss_fn(tgt_preds, tgt_labels)\n",
    "        D_loss = D_src_loss + D_tgt_loss\n",
    "\n",
    "        discriminator_optim.zero_grad()\n",
    "        D_loss.backward()\n",
    "        discriminator_optim.step()\n",
    "\n",
    "        # update the target encoder\n",
    "\n",
    "        # tgt encoder tries to fool the discriminator by making it output 1 for tgt features\n",
    "        tgt_labels = torch.full((tgt_x.shape[0],), fill_value=1, dtype=torch.long, device=device)\n",
    "        tgt_preds = discriminator(tgt_features)\n",
    "\n",
    "        Mt_loss = loss_fn(tgt_preds, tgt_labels)\n",
    "\n",
    "        tgt_model_optim.zero_grad()\n",
    "        Mt_loss.backward()\n",
    "        tgt_model_optim.step()\n",
    "\n",
    "        if checkpoint and step % checkpoint == 0:\n",
    "            val_loss, val_acc = evaluate(tgt_model, tgt_val_loader, loss_fn)\n",
    "\n",
    "            train_stats += [[step, D_src_loss.item(), D_tgt_loss.item(), Mt_loss.item()]]\n",
    "            val_stats += [[step, val_loss, val_acc]]\n",
    "\n",
    "            tb_writer.add_scalar('adapt/adv_loss/D_src', D_src_loss.item(), step)\n",
    "            tb_writer.add_scalar('adapt/adv_loss/D_tgt', D_tgt_loss.item(), step)\n",
    "            tb_writer.add_scalar('adapt/adv_loss/Mt', Mt_loss.item(), step)\n",
    "\n",
    "            tb_writer.add_scalar('adapt/val/tgt_loss', val_loss, step)\n",
    "            tb_writer.add_scalar('adapt/val/tgt_acc', val_acc, step)\n",
    "\n",
    "            # save the best model\n",
    "            # this may be cheating since in reality the target dataset is unlabeled\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(tgt_model.state_dict(), log_dir/'tgt_best_model.pth')\n",
    "    \n",
    "    tb_writer.close()\n",
    "\n",
    "    train_stats = np.array(train_stats)\n",
    "    val_stats = np.array(val_stats)\n",
    "    return (train_stats, val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats, val_stats = train_tgt_encoder(\n",
    "    src_model,\n",
    "    tgt_model,\n",
    "    discriminator,\n",
    "    src_train,\n",
    "    tgt_train,\n",
    "    tgt_val,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    torch.optim.Adam(tgt_model.parameters(), lr=1e-4, betas=(0.5, 0.9)),\n",
    "    torch.optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.5, 0.9)),\n",
    "    num_steps=5000,\n",
    "    checkpoint=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 1], label='D_src_loss')\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 2], label='D_tgt_loss')\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 3], label='Mt_loss')\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 1], label='Mt_val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 2], label='Mt_val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Combine the target encoder and the source classifier\n",
    "\n",
    "The combination of the target encoder and the source classifier has already been done in the previous step through weight copying. We can now evaluate the overall classifier on the target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_model.load_state_dict(torch.load(log_dir/'tgt_best_model.pth'))\n",
    "\n",
    "src_loss, src_acc = evaluate(tgt_model, src_test, nn.CrossEntropyLoss())\n",
    "tgt_loss, tgt_acc = evaluate(tgt_model, tgt_test, nn.CrossEntropyLoss())\n",
    "\n",
    "print('Adapted model:')\n",
    "print(f'src_test -> loss: {src_loss:.4f}, acc: {src_acc:.4f}')\n",
    "print(f'tgt_test -> loss: {tgt_loss:.4f}, acc: {tgt_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
