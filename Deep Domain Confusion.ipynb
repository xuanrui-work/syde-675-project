{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Domain Confusion (DDC)\n",
    "\n",
    "This is the implementation for the following paper:\n",
    "```\n",
    "@article{tzeng2014deep,\n",
    "  title={Deep domain confusion: Maximizing for domain invariance},\n",
    "  author={Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},\n",
    "  journal={arXiv preprint arXiv:1412.3474},\n",
    "  year={2014}\n",
    "}\n",
    "```\n",
    "\n",
    "The paper is also available [here](./res/Deep%20Domain%20Confusion%20-%20Maximizing%20for%20Domain%20Invariance.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _base_model\n",
    "import _dataloader_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For logging multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "log_root = pathlib.Path(f'./ddc')\n",
    "run_id = 1\n",
    "while True:\n",
    "    log_dir = log_root/f'run{run_id}'\n",
    "    if not log_dir.exists():\n",
    "        break\n",
    "    run_id += 1\n",
    "\n",
    "# use a previous run\n",
    "# log_dir = log_root/'run1'\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Logging to: {log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "\n",
    "Here, we transform both the source and target datasets into the same size, and repeat the channel dimension for grayscale images, such that both datasets have the same input shape to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 32, 32)\n",
    "num_classes = 10\n",
    "\n",
    "loader_helper = _dataloader_helper.MNIST2USPS(image_size=input_shape[1:])\n",
    "src_train, src_val, src_test = loader_helper.get_src_loaders()\n",
    "tgt_train, tgt_val, tgt_test = loader_helper.get_tgt_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some samples from both domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "src_samples = next(iter(src_train))\n",
    "tgt_samples = next(iter(tgt_train))\n",
    "\n",
    "src_images, src_labels = src_samples[0][:num_samples], src_samples[1][:num_samples]\n",
    "tgt_images, tgt_labels = tgt_samples[0][:num_samples], tgt_samples[1][:num_samples]\n",
    "\n",
    "def plot_samples(samples, figsize=(16, 6)):\n",
    "    fig, ax = plt.subplots(1, num_samples, figsize=figsize)\n",
    "    for i in range(num_samples):\n",
    "        ax[i].imshow(np.transpose(samples[i], (1, 2, 0)))\n",
    "        ax[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(src_images)\n",
    "print(src_images.shape)\n",
    "print(src_labels)\n",
    "\n",
    "plot_samples(tgt_images)\n",
    "print(tgt_images.shape)\n",
    "print(tgt_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDC\n",
    "\n",
    "The domain adaptation schema of DDC is outlined in the figure below (note that the model architecture shown below differs from our implementation):<br>\n",
    "<img src=\"./res/images/ddc-overview.png\" width=\"500\">\n",
    "\n",
    "It achieves alignment bewteen the source and target domain by minimizing the maximium mean discrepancy (MMD) between the deep features from the respective domains. The MMD is a measure of the distance between two distributions, whose empirical estimate is given by:\n",
    "$$\\begin{align*}\n",
    "\\text{MMD}(X_S, X_T) &= \\left\\| \\frac{1}{|X_S|} \\sum_{x_s \\in X_S} \\phi(x_s) - \\frac{1}{|X_T|} \\sum_{x_t \\in X_T} \\phi(x_t) \\right\\|\n",
    "\\end{align*}$$\n",
    "where $X_S$ and $X_T$ are datasets for the source and target domains respectively, and $\\phi(x)$ is the deep feature representation of the input $x$.\n",
    "\n",
    "The overall loss is then given by the combination of the classification loss and the MMD loss:\n",
    "$$\\begin{align*}\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{cls}}(X_S, Y_S) + \\lambda \\ \\text{MMD}^2(X_S, X_T)\n",
    "\\end{align*}$$\n",
    "where $\\mathcal{L}_{\\text{cls}}$ is the classification loss, and $\\lambda$ is a hyperparameter that controls the balance between the two losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _base_model.CNNClassifier(\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "summary(model, input_size=(5, *input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, loss_fn):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (x, y) in enumerate(val_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred, _ = model(x)\n",
    "            loss = loss_fn(y_pred, y).item()\n",
    "            n_correct = (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            val_loss += loss * x.shape[0]\n",
    "            val_acc += n_correct\n",
    "    model.train(training)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "    return (val_loss, val_acc)\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    src_loader,\n",
    "    tgt_loader,\n",
    "    src_val_loader,\n",
    "    tgt_val_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    num_steps=20000,\n",
    "    alpha=1,\n",
    "    checkpoint=100,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    train_stats = []\n",
    "    val_stats = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    src_loader_iter = iter(src_loader)\n",
    "    tgt_loader_iter = iter(tgt_loader)\n",
    "\n",
    "    tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        try:\n",
    "            src_x, src_y = next(src_loader_iter)\n",
    "        except StopIteration:\n",
    "            src_loader_iter = iter(src_loader)\n",
    "            src_x, src_y = next(src_loader_iter)\n",
    "        try:\n",
    "            tgt_x, tgt_y = next(tgt_loader_iter)\n",
    "        except StopIteration:\n",
    "            tgt_loader_iter = iter(tgt_loader)\n",
    "            tgt_x, tgt_y = next(tgt_loader_iter)\n",
    "        \n",
    "        src_x, src_y = src_x.to(device), src_y.to(device)\n",
    "        tgt_x, tgt_y = tgt_x.to(device), tgt_y.to(device)\n",
    "\n",
    "        src_y_pred, src_features = model(src_x)\n",
    "        tgt_y_pred, tgt_features = model(tgt_x)\n",
    "\n",
    "        # note that we do not use tgt_y\n",
    "        loss_cls = loss_fn(src_y_pred, src_y)\n",
    "\n",
    "        # the MMD loss\n",
    "        loss_mmd = torch.linalg.norm(\n",
    "            1/src_features.shape[0] * torch.sum(src_features, axis=0) -\n",
    "            1/tgt_features.shape[0] * torch.sum(tgt_features, axis=0)\n",
    "        )**2\n",
    "\n",
    "        loss = loss_cls + alpha*loss_mmd\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % checkpoint == 0:\n",
    "            src_val_loss, src_val_acc = evaluate(model, src_val_loader, loss_fn)\n",
    "            tgt_val_loss, tgt_val_acc = evaluate(model, tgt_val_loader, loss_fn)\n",
    "\n",
    "            train_stats += [[\n",
    "                step,\n",
    "                loss.item(),\n",
    "                loss_cls.item(),\n",
    "                loss_mmd.item(),\n",
    "            ]]\n",
    "            val_stats += [[\n",
    "                step,\n",
    "                src_val_loss,\n",
    "                src_val_acc,\n",
    "                tgt_val_loss,\n",
    "                tgt_val_acc,\n",
    "            ]]\n",
    "\n",
    "            tb_writer.add_scalar('train/loss', loss.item(), step)\n",
    "            tb_writer.add_scalar('train/loss_cls', loss_cls.item(), step)\n",
    "            tb_writer.add_scalar('train/loss_mmd', loss_mmd.item(), step)\n",
    "\n",
    "            tb_writer.add_scalar('val/src_loss_cls', src_val_loss, step)\n",
    "            tb_writer.add_scalar('val/src_acc', src_val_acc, step)\n",
    "            tb_writer.add_scalar('val/tgt_loss_cls', tgt_val_loss, step)\n",
    "            tb_writer.add_scalar('val/tgt_acc', tgt_val_acc, step)\n",
    "\n",
    "            # save the best model\n",
    "            # this may be cheating since in reality the target dataset is unlabeled\n",
    "            if tgt_val_loss < best_val_loss:\n",
    "                best_val_loss = tgt_val_loss\n",
    "                torch.save(model.state_dict(), log_dir/'tgt_best_model.pth')\n",
    "    \n",
    "    tb_writer.close()\n",
    "\n",
    "    train_stats = np.array(train_stats)\n",
    "    val_stats = np.array(val_stats)\n",
    "    return (train_stats, val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats, val_stats = train(\n",
    "    model,\n",
    "    src_train,\n",
    "    tgt_train,\n",
    "    src_val,\n",
    "    tgt_val,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.5, 0.9)),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    num_steps=5000,\n",
    "    alpha=0.25,\n",
    "    checkpoint=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 18))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 1], label='loss')\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 2], label='loss_cls')\n",
    "plt.plot(train_stats[:, 0], train_stats[:, 3], label='loss_mmd')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Loss')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 1], label='src_loss')\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 3], label='tgt_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Validation Loss')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 2], label='src_acc')\n",
    "plt.plot(val_stats[:, 0], val_stats[:, 4], label='tgt_acc')\n",
    "plt.legend()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(log_dir/'tgt_best_model.pth'))\n",
    "\n",
    "src_loss, src_acc = evaluate(model, src_test, nn.CrossEntropyLoss())\n",
    "tgt_loss, tgt_acc = evaluate(model, tgt_test, nn.CrossEntropyLoss())\n",
    "\n",
    "print('Adapted model:')\n",
    "print(f'src_test -> loss: {src_loss:.4f}, acc: {src_acc:.4f}')\n",
    "print(f'tgt_test -> loss: {tgt_loss:.4f}, acc: {tgt_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
